{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import cv2\n",
    "import time\n",
    "import json\n",
    "import errno\n",
    "import random\n",
    "import torch\n",
    "import openai\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from glob import iglob\n",
    "from PIL import Image\n",
    "\n",
    "from functools import reduce\n",
    "from shutil import rmtree\n",
    "from typing import Optional, Tuple, Dict, List\n",
    "from os.path import join as pjoin\n",
    "\n",
    "from torch.utils.data import Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkdir_p(path):\n",
    "    try:\n",
    "        os.makedirs(path)\n",
    "    except OSError as exc:\n",
    "        if exc.errno == errno.EEXIST and os.path.isdir(path):\n",
    "            pass\n",
    "        else:\n",
    "            raise\n",
    "def del_folder(path):\n",
    "    try:\n",
    "        rmtree(path)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "def read_json(json_path):\n",
    "    with open(json_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def write_json(save_path, json_obj):\n",
    "    with open(save_path, 'w', encoding='utf-8') as make_file:\n",
    "        json.dump(json_obj, make_file, indent=\"\\t\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"/data/ai_hub/dst/\"\n",
    "ANNOT_DIR = pjoin(DATA_DIR, \"annotations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "274"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_annot = read_json(pjoin(ANNOT_DIR, \"dev.json\"))\n",
    "len(dev_annot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 311,\n",
       " 'filename': '25ba191dd4beed1d666725510ccc4a968fdcb567dac81adba024c720d82414af_남_20_중립_문화재 및 유적지_20201204233820-009-005.jpg',\n",
       " 'age': 20,\n",
       " 'sex': 'male',\n",
       " 'emotion': 'neutral',\n",
       " 'dialogue': [{'turn_id': 0,\n",
       "   'system': 'How are you feeling today?',\n",
       "   'user': \"I'm feeling alright \",\n",
       "   'belief_state': {}}]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_annot[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, AutoModelForCausalLM\n",
    "\n",
    "class MultimodalDstDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 json_path, \n",
    "                 vision_dir, \n",
    "                 processor=None, \n",
    "                 model_path=None, \n",
    "                 max_length=512, \n",
    "                 seed=19\n",
    "                 ):\n",
    "        self.data = read_json(json_path)\n",
    "        self.start_of_belief = \"=> Belief State:\"\n",
    "        self.eob = \"[EOB]\"\n",
    "\n",
    "        self.processor = processor if not model_path else AutoProcessor.from_pretrained(model_path)\n",
    "        self.vision_dir = vision_dir\n",
    "        self.max_length = max_length\n",
    "\n",
    "        random.seed(seed)\n",
    "        random.shuffle(self.data)\n",
    "        \n",
    "        self.build_dataset()        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def build_dataset(self):\n",
    "        predicts, targets, visions = [], [], []\n",
    "        \n",
    "        for content in self.data:\n",
    "            dialog = content['dialogue']\n",
    "            uid = content[\"filename\"].split(\"_\")[0]\n",
    "            upload_id = content[\"filename\"].split(\"_\")[-1]\n",
    "            \n",
    "            filename = \"_\".join([uid, content[\"sex\"], str(content[\"age\"]), content[\"emotion\"], upload_id])\n",
    "\n",
    "            user_state = dict(user_state=content[\"emotion\"])\n",
    "            context = \"User:\"\n",
    "            predicts += [f\"{context} {self.start_of_belief}\"]\n",
    "            targets += [f\"{context} {self.start_of_belief} {str(user_state)} {self.eob} System: {dialog[0]['system']}\"]\n",
    "\n",
    "            for i in range(len(dialog) - 1):\n",
    "                # Update context\n",
    "                context += f\" System: {dialog[i]['system']}\" + f\" User: {dialog[i]['user']}\"\n",
    "                belief_state = dialog[i]['belief_state']\n",
    "                \n",
    "                visions += [filename]\n",
    "                predicts += [f\"{context} {self.start_of_belief}\"]\n",
    "                targets += [f\"{context} {self.start_of_belief} {str(belief_state)} {self.eob} System: {dialog[i + 1]['system']}\"]\n",
    "        \n",
    "        assert len(predicts) == len(targets)\n",
    "\n",
    "        input_ = self.processor.tokenizer.batch_encode_plus(\n",
    "            predicts, add_special_tokens=True, max_length=self.max_length, truncation=True\n",
    "        )\n",
    "        output_ = self.processor.tokenizer.batch_encode_plus(\n",
    "            targets, add_special_tokens=True, max_length=self.max_length, truncation=True\n",
    "        )\n",
    "        self.inputs = input_[\"input_ids\"]\n",
    "        self.input_masks = input_[\"attention_mask\"]\n",
    "        self.labels = output_[\"input_ids\"]  \n",
    "        self.visions = visions\n",
    "\n",
    "    def __getitem__(self, index):        \n",
    "        image_path = pjoin(self.vision_dir, self.visions[index])\n",
    "        try:\n",
    "            # Process image\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "            pixel_values = self.processor(images=image, return_tensors=\"pt\").pixel_values\n",
    "        except:\n",
    "            print(f\"Failed to load examples with image: {image_path}. \")\n",
    "\n",
    "        return dict(\n",
    "            pixel_values=pixel_values,\n",
    "            input_ids=self.inputs[index],\n",
    "            attention_mask=self.input_masks[index],\n",
    "            labels=self.labels[index],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_DIR = \"/data/ai_hub/images/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_dataset = MultimodalDstDataset(json_path=pjoin(ANNOT_DIR, \"dev.json\"),\n",
    "                        vision_dir=IMAGE_DIR,\n",
    "                        processor=None, \n",
    "                        model_path=\"microsoft/git-base\", \n",
    "                        max_length=512, \n",
    "                        seed=19\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "988"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dev_dataset.inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MultimodalDstDataset(json_path=pjoin(ANNOT_DIR, \"train.json\"),\n",
    "                        vision_dir=IMAGE_DIR,\n",
    "                        processor=None, \n",
    "                        model_path=\"microsoft/git-base\", \n",
    "                        max_length=512, \n",
    "                        seed=19\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2993"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset.inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tod",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
